{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'readability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d402a383cef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreadability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m text = \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Introduction to PyMC3 - Concept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'readability'"
     ]
    }
   ],
   "source": [
    "from readability import Readability\n",
    "\n",
    "text = \"\"\"\n",
    "# Introduction to PyMC3 - Concept \n",
    "\n",
    "In this lecture we're going to talk about the theories, advantages and application of using Bayesian\n",
    "statistics. \n",
    "\n",
    "Bayesian inference is a method of making statistical decisions as more information or evidence becomes\n",
    "available. It is a powerful tool to help us understand uncertain events in the form of probabilities.\n",
    "\n",
    "Central to the application of Bayesian methods is the idea of a distribution. In general, a distribution\n",
    "describes how likely a certain outcome will occur given a series of inputs. For instance, we might have a\n",
    "distribution which describes how likely a given kicker on a football team will make a field goal (the\n",
    "outcome).\n",
    "\n",
    "# We can use the line magic %run to load all variables from the previous Overview lecture\n",
    "# and revisit the histogram\n",
    "%run PyMC3_Code_Training_Overview.ipynb\n",
    "\n",
    "Here is an example of what the distribution of National Football League kicker's field goal probability\n",
    "looks like. It shows that most kickers score roughly 85 percent of the field goals and the number drops\n",
    "gradually on both sides as field goal percentage goes up and down. So the graph looks pretty much a bell\n",
    "curve, a shape reminiscent of a bell.\n",
    "\n",
    "In Bayesian statistics, we consider our initial understanding of the outcome of an event before data is\n",
    "collected. We do this by creating a **prior distribution**. Similar to most data science methods, we will\n",
    "then collect the observed data to create a model using some probability distribution. We rely on that model\n",
    "called **likelihood** to update our initial understanding (**prior distribution**) using the **Bayes'\n",
    "Theorem**.\n",
    "\n",
    "We won't go into the maths of Bayes' theorem, but the theorem suggests that by using the observed data as\n",
    "new evidence, the updated belief incorporates our initial understanding and the evidence related to the\n",
    "problem being examined, just like we cumulate knowledge by learning the problems or doing experiments.\n",
    "**Bayes's theorem** also suggests a nice property that the updated belief is proportional to the product of\n",
    "the **prior distribution** and the **likelihood**. We call the updated belief **posterior distribution**. \n",
    "\n",
    "![](Bayesian_Model.png)\n",
    "\n",
    "Bayesian inference is carried out in four steps. \n",
    "\n",
    "First, specify prior distribution. We can start with forming some prior distribution for our initial\n",
    "understanding of a subject matter, before observing data. This **prior distribution** can come from personal\n",
    "experience, literature, or even thoughts from experts.\n",
    "\n",
    "During the second and third step, we move on to **collect the data** from observations or experiments and\n",
    "then **construct a model** using probability distribution(s) to represent how likely the outcomes occur\n",
    "given the data input. In our example, we can collect evidence by recording American football players' field\n",
    "goal performance for a few NFL seasons in a database. Then based on the data we collect, we can use\n",
    "visualizations to understand how to construct a suitable probabilistic model (**likelihood**) to represent\n",
    "the distribution of the NFL data.\n",
    "\n",
    "Finally, apply the Bayes' rules. In this step, we incorporate the **prior distribution** and the\n",
    "**likelihood** using Bayes' rules to update our understanding and return the **posterior distribution**. \n",
    "\n",
    "Let's first focus on the first three steps. \n",
    "\n",
    "For the NFL example, as we observe kicks from a player, we can record the success and failures for each kick\n",
    "and then assign binomial distribution to represent the how likely a field goal is made for different\n",
    "players. We usually use **beta distribution** to set up a prior distribution involving success and failure\n",
    "trials, where you can specify the parameters to decide how likely a player will make a field goal based on\n",
    "your knowledge.\n",
    "\n",
    "# We can randomly generate binomial distributed samples by \n",
    "# setting the sample size as 100 and success probability of making a field goal as 0.8\n",
    "n = 100\n",
    "p = 0.8\n",
    "size = 100\n",
    "# Next we can set up binom as a random binomial variable that will give us 100 draws of binomial distributions\n",
    "binom = np.random.binomial(n, p, size)\n",
    "\n",
    "# Now we use plt.figure function to define the figure size and plt.hist to draw a histogram, so you \n",
    "# can visualize what's going on here\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(binom/n)\n",
    "# Let's set the x and y axis and the title as well.\n",
    "plt.xlabel(\"Success Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Binomial Distribution\")\n",
    "# and show the plot\n",
    "plt.show()\n",
    "\n",
    "Once again, you can see from this graph that most samples made roughly 80% of the field goal, with only few\n",
    "of those making less than 72.5% and more than 87.5%. We can again say the histogram shows a binomial\n",
    "distribution with mean closed to 0.8.\n",
    "\n",
    "## Posterior Distribution\n",
    "\n",
    "The question of how to obtain the posterior distribution by Bayes' rules represents the coolest part of\n",
    "Bayesian analysis.\n",
    "\n",
    "Traditional methods include integrating the random variables and determining the resulting distribution in\n",
    "closed form. However, it is not always the case that posterior distribution is obtainable through\n",
    "integration. \n",
    "\n",
    "**Probabilistic programming languages**, a renowned programming tool to perform probabilistic models\n",
    "automatically, can help update the belief iteratively to approximate the posterior distribution even when a\n",
    "model is complex or hierarchical. For example, the NUTS algorithm under Monte Carlo Markov Chain (MCMC),\n",
    "which we will discuss in the next video, is found to be effective in computing and representing the\n",
    "posterior distribution. \n",
    "\n",
    "## Bayesian Inference Advantages\n",
    "\n",
    "![](Bayesian_Advantage.png)\n",
    "\n",
    "So why Bayesian Inference useful? A salient advantage of the Bayesian approach in statistics lies on its\n",
    "capability of easily cumulating knowledge. For every time when you have new evidence, using Bayesian\n",
    "approach can effectively update our prior knowledge. \n",
    "\n",
    "Another potential of Bayesian statistics is that it allows every parameters, such as the probability of\n",
    "making the field goal, be summarized by probability distributions, regardless of whether it is prior,\n",
    "likelihood, or posterior. With that, we can always look at the distributions to quantify the degree of\n",
    "uncertainty. \n",
    "\n",
    "Furthermore, Bayesian approach generates more robust posterior statistics by utilizing flexible\n",
    "distributions in prior and the observed data.\n",
    "\n",
    "In the next video, we will introduce PyMC3, a Python package to perform MCMC conveniently and obtain easily\n",
    "interpretable Bayesian estimations and plots, with a worked example. Bye for now!\n",
    "\n",
    "\"\"\"\n",
    "r = Readability(text)\n",
    "\n",
    "r.flesch_kincaid()\n",
    "# r.flesch()\n",
    "# r.gunning_fog()\n",
    "# r.coleman_liau()\n",
    "# r.dale_chall()\n",
    "# r.ari()\n",
    "# r.linsear_write()\n",
    "# r.smog()\n",
    "# r.spache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
